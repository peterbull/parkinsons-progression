# AUTOGENERATED! DO NOT EDIT! File to edit: ../pb_parkinsons_prog.ipynb.

# %% auto 0
__all__ = ['comp', 'path', 'df_train_proteins', 'df_train_clinical', 'df_train_peptides', 'df_train_supplemental',
           'df_test_peptides', 'df_test_updrs', 'df_train', 'median_targs', 'dep_var', 'procs', 'cont', 'cat', 'splits',
           'to', 'dls', 'xs', 'ys', 'valid_xs', 'valid_ys', 'learn', 'MultiTargetSMAPE']

# %% ../pb_parkinsons_prog.ipynb 7
from fastai.tabular.all import *

pd.options.display.max_rows = 20
pd.options.display.max_columns = 8

# %% ../pb_parkinsons_prog.ipynb 8
try: import fastkaggle
except ModuleNotFoundError:
    !pip install -Uq fastkaggle

from fastkaggle import *

# %% ../pb_parkinsons_prog.ipynb 10
comp = 'amp-parkinsons-disease-progression-prediction'
path = setup_comp(comp, install='fastai')

# %% ../pb_parkinsons_prog.ipynb 13
df_train_proteins = pd.read_csv(path/"train_proteins.csv", low_memory=False)
df_train_clinical = pd.read_csv(path/"train_clinical_data.csv", low_memory=False)
df_train_peptides = pd.read_csv(path/"train_peptides.csv", low_memory=False)
df_train_supplemental = pd.read_csv(path/"supplemental_clinical_data.csv", low_memory=False)

# %% ../pb_parkinsons_prog.ipynb 15
df_train_proteins = pd.read_csv(path/"example_test_files/test_proteins.csv", low_memory=False)
df_test_peptides = pd.read_csv(path/"example_test_files/test_peptides.csv", low_memory=False)
df_test_updrs = pd.read_csv(path/"example_test_files/test.csv", low_memory=False)

# %% ../pb_parkinsons_prog.ipynb 18
df_train = df_train_proteins.merge(df_train_clinical, on=['patient_id', 'visit_id', 'visit_month'], how='left')

# %% ../pb_parkinsons_prog.ipynb 20
median_targs = df_train[['updrs_1', 'updrs_2', 'updrs_3', 'updrs_4']].median()

# %% ../pb_parkinsons_prog.ipynb 21
df_train[['updrs_1', 'updrs_2', 'updrs_3', 'updrs_4']] = df_train[['updrs_1', 'updrs_2', 'updrs_3', 'updrs_4']].fillna(median_targs)

# %% ../pb_parkinsons_prog.ipynb 23
dep_var = ['updrs_1', 'updrs_2', 'updrs_3', 'updrs_4']

# %% ../pb_parkinsons_prog.ipynb 25
procs = [Categorify, FillMissing, Normalize]
cont, cat = cont_cat_split(df_train, dep_var=dep_var, max_card=1)
splits = RandomSplitter(valid_pct=0.2)(range_of(df_train))

# %% ../pb_parkinsons_prog.ipynb 27
to = TabularPandas(df_train, procs, cat, cont, y_names=dep_var, splits=splits)

# %% ../pb_parkinsons_prog.ipynb 29
class MultiTargetSMAPE(Metric):
    def __init__(self):
        super().__init__()
    
    def reset(self):
        self.total = 0.
        self.count = 0
        
    def accumulate(self, learn):
        pred,targ = learn.pred, learn.y
        denom = torch.abs(pred) + torch.abs(targ)
        non_zero_denom = denom != 0
        num = torch.abs(pred - targ)
        smape = torch.zeros_like(num)
        smape[non_zero_denom] = num[non_zero_denom] / denom[non_zero_denom]
        self.total += smape.sum(dim=0)
        self.count += learn.y.size(0)
    
    @property
    def value(self):
        return (self.total / self.count).mean().item() * 100  # SMAPE in percentage
    
    @property
    def name(self):
        return 'multi_target_smape'


# %% ../pb_parkinsons_prog.ipynb 31
dls = to.dataloaders(bs=256)

# %% ../pb_parkinsons_prog.ipynb 32
xs, ys = to.train.xs, to.train.ys
valid_xs, valid_ys = to.valid.xs, to.valid.ys

# %% ../pb_parkinsons_prog.ipynb 34
learn = tabular_learner(dls, layers=[200,100], metrics=[MultiTargetSMAPE()], n_out=4, y_range=(0, 80), loss_func=mse)

# %% ../pb_parkinsons_prog.ipynb 38
learn.fit_one_cycle(10, 1e-3)
