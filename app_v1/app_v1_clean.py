# AUTOGENERATED! DO NOT EDIT! File to edit: ../pb_parkinsons_prog.ipynb.

# %% auto 0
__all__ = ['comp', 'path', 'protein_train_df', 'clinical_train_df', 'peptide_train_df', 'supplement_train_df', 'protein_test_df',
           'peptide_test_df', 'updrs_test_df', 'train_df', 'median_targs', 'dep_var', 'procs', 'to', 'dls', 'learn',
           'xs', 'ys', 'valid_xs', 'valid_ys', 'MultiTargetSMAPE']

from fastai.tabular.all import *

import seaborn as sns

import tqdm

pd.options.display.max_rows = 20
pd.options.display.max_columns = 8

try: import fastkaggle
except ModuleNotFoundError:
    !pip install -Uq fastkaggle

from fastkaggle import *

comp = 'amp-parkinsons-disease-progression-prediction'
path = setup_comp(comp, install='fastai')

protein_train_df = pd.read_csv(path/"train_proteins.csv", low_memory=False)
clinical_train_df = pd.read_csv(path/"train_clinical_data.csv", low_memory=False)
peptide_train_df = pd.read_csv(path/"train_peptides.csv", low_memory=False)
supplement_train_df = pd.read_csv(path/"supplemental_clinical_data.csv", low_memory=False)

protein_test_df = pd.read_csv(path/"example_test_files/test_proteins.csv", low_memory=False)
peptide_test_df = pd.read_csv(path/"example_test_files/test_peptides.csv", low_memory=False)
updrs_test_df = pd.read_csv(path/"example_test_files/test.csv", low_memory=False)

train_df = peptide_train_df.merge(protein_train_df, on=['patient_id', 'visit_id', 'visit_month', 'UniProt'], how='left')
train_df = train_df.merge(clinical_train_df, on=['patient_id', 'visit_id', 'visit_month'], how='left')

median_targs = df_train[['updrs_1', 'updrs_2', 'updrs_3', 'updrs_4']].median()

df_train[['updrs_1', 'updrs_2', 'updrs_3', 'updrs_4']] = df_train[['updrs_1', 'updrs_2', 'updrs_3', 'updrs_4']].fillna(median_targs)

dep_var = ['updrs_1', 'updrs_2', 'updrs_3', 'updrs_4']

procs = [Categorify, FillMissing, Normalize]


to = TabularPandas(df_train, procs, cat, cont, y_names=dep_var, splits=splits)

class MultiTargetSMAPE(Metric):
    def __init__(self):
        super().__init__()
    
    def reset(self):
        self.total = 0.
        self.count = 0
        
    def accumulate(self, learn):
        pred,targ = learn.pred, learn.y
        denom = torch.abs(pred) + torch.abs(targ)
        non_zero_denom = denom != 0
        num = torch.abs(pred - targ)
        smape = torch.zeros_like(num)
        smape[non_zero_denom] = num[non_zero_denom] / denom[non_zero_denom]
        self.total += smape.sum(dim=0)
        self.count += learn.y.size(0)
    
    @property
    def value(self):
        return (self.total / self.count).mean().item() * 100  # SMAPE in percentage
    
    @property
    def name(self):
        return 'multi_target_smape'


dls = to.dataloaders(bs=256)

learn = tabular_learner(dls, layers=[200,100], metrics=[MultiTargetSMAPE()], n_out=4, y_range=(0, 80), loss_func=mse)

learn.fit_one_cycle(10, 1e-3)

xs, ys = to.train.xs, to.train.ys
valid_xs, valid_ys = to.valid.xs, to.valid.ys
